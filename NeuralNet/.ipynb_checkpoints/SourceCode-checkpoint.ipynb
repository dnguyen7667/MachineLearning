{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, depth = 1, width = 1): \n",
    "        \"\"\"\n",
    "        depth = how many layers\n",
    "        width = how many nodes/layer\n",
    "        \"\"\"\n",
    "        self.layers = layers # number of hidden layers: int\n",
    "        self.nodes_per_layer = nodes_per_layer #number of nodes per layer : 1 by (number of layers) vector\n",
    "    \n",
    "    def subff(X, W, bias):\n",
    "        # n = size of input X\n",
    "        # dim of W(eights) : (num_node_layer) by (n)\n",
    "        X = X.reshape(X.shape[0],1) # dim: n by 1\n",
    "        return W.dot(X) + bias # W.X\n",
    "    \n",
    "    def sigmoid(X): # activation function\n",
    "        return 1/(1 + np.e**-X)\n",
    "\n",
    "\n",
    "    def update_weight(w_in, slope, learning_rate):\n",
    "        return w_in - learning_rate*slope \n",
    "\n",
    "    \n",
    "\n",
    "    def partial_derivaties_XY_thru_layer(X, Y, W): # This is just the weights exact\n",
    "        # dim: (num_of_nodes) by (num of total_weights)\n",
    "        return W\n",
    "    \n",
    "    def dydx(x, function = 'sigmoid'):\n",
    "    y = 1 / (1+ np.e**-x)\n",
    "    return y*(1-y)\n",
    "\n",
    "\n",
    "    # matrix to store all partial derivatives of Y wrt X at each layer \n",
    "    # dY/dX : partial derivatives of Y wrt to X\n",
    "    # with Y is a result of transforming X thru sigmoid or other activation function\n",
    "\n",
    "    def partial_derivaties_YX_within(X, Y, W):\n",
    "        mat = np.zeros((X.shape[0], X.shape[0]))\n",
    "        for i in range(X.shape[0]):\n",
    "            # Only the diagonals are none-zero (most cases)\n",
    "            mat[i][i] = dydx(X[i])\n",
    "\n",
    "\n",
    "        return mat\n",
    "    \n",
    "\n",
    "    # matrix to store all partial derivatives of X wrt Y at each layer \n",
    "    # dX/dY : partial derivatives \"thru layers\" == Weight w_i+1 that used for \"transfer\" layer from y_i to x_i+1\n",
    "\n",
    "\n",
    "\n",
    "    # treat input as a first \n",
    "    def partial_derivaties_XW(X, Y, W): # dX/dY are the prev Y values\n",
    "        mat = np.zeros((X.shape[0], X.shape[0]*Y.shape[0]))\n",
    "        num = Y.shape[0]\n",
    "\n",
    "        for i in range(mat.shape[0]):\n",
    "            # each row stores partial derivatives of X wrt to all the weights in that layer,\n",
    "            # which mostly will be 0 except for the values that match with the initial weights assigned to that node\n",
    "             # number of rows of W\n",
    "            mat[i][i*num : i*num + num] = Y.ravel()\n",
    "\n",
    "\n",
    "        return mat\n",
    "\n",
    "\n",
    "\n",
    "    def cost_mat(predictions, targets, func = 'mse'):\n",
    "        cost = 1/2 * sum((predictions - targets)**2)\n",
    "        cost_mat = np.array(predictions - targets)\n",
    "        return cost_mat.T\n",
    "\n",
    "    def cost_func(y_pred, y_target): # calculation of cost/ loss\n",
    "        return 1/2 * sum((y_pred-y_target)**2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def subff(X, W, bias):\n",
    "    # n = size of input X\n",
    "    # dim of W(eights) : (num_node_layer) by (n)\n",
    "    X = X.reshape(X.shape[0],1) # dim: n by 1\n",
    "    return W.dot(X) + bias # W.X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X): # activation function\n",
    "    return 1/(1 + np.e**-X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weight(w_in, slope, learning_rate):\n",
    "    return w_in - learning_rate*slope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of derivatives of dX/dw, dX/dy\n",
    "# dY/dX\n",
    "\n",
    "# At very step calculating new input to a hidden layer\n",
    "# calculate the partial derivatives of that input wrt prev weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix to store all partial derivatives of X wrt W at each laye\n",
    "\n",
    "def partial_derivaties_XY_thru_layer(X, Y, W): # This is just the weights exact\n",
    "    # dim: (num_of_nodes) by (num of total_weights)\n",
    "   \n",
    "    return W\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dydx(x, function = 'sigmoid'):\n",
    "    y = 1 / (1+ np.e**-x)\n",
    "    return y*(1-y)\n",
    "\n",
    "\n",
    "# matrix to store all partial derivatives of Y wrt X at each layer \n",
    "# dY/dX : partial derivatives of Y wrt to X\n",
    "# with Y is a result of transforming X thru sigmoid or other activation function\n",
    "\n",
    "def partial_derivaties_YX_within(X, Y, W):\n",
    "    mat = np.zeros((X.shape[0], X.shape[0]))\n",
    "    for i in range(X.shape[0]):\n",
    "        # Only the diagonals are none-zero (most cases)\n",
    "        mat[i][i] = dydx(X[i])\n",
    "        \n",
    "    \n",
    "    return mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix to store all partial derivatives of X wrt Y at each layer \n",
    "# dX/dY : partial derivatives \"thru layers\" == Weight w_i+1 that used for \"transfer\" layer from y_i to x_i+1\n",
    "\n",
    "\n",
    "\n",
    "# treat input as a first \n",
    "def partial_derivaties_XW(X, Y, W): # dX/dY are the prev Y values\n",
    "    mat = np.zeros((X.shape[0], X.shape[0]*Y.shape[0]))\n",
    "    num = Y.shape[0]\n",
    "    \n",
    "    for i in range(mat.shape[0]):\n",
    "        # each row stores partial derivatives of X wrt to all the weights in that layer,\n",
    "        # which mostly will be 0 except for the values that match with the initial weights assigned to that node\n",
    "         # number of rows of W\n",
    "        mat[i][i*num : i*num + num] = Y.ravel()\n",
    "    \n",
    "        \n",
    "    return mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cost_mat(predictions, targets, func = 'mse'):\n",
    "    cost = 1/2 * sum((predictions - targets)**2)\n",
    "    cost_mat = np.array(predictions - targets)\n",
    "    return cost_mat.T\n",
    "    \n",
    "def cost_func(y_pred, y_target): # calculation of cost/ loss\n",
    "    return 1/2 * sum((y_pred-y_target)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(y_pred, y_target): # calculation of cost/ loss\n",
    "    return 1/2 * sum((y_pred-y_target)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward\n",
    "\n",
    "#feed forward to first layer\n",
    "x_arr = []\n",
    "y_arr = []\n",
    "w_arr = []\n",
    "learning_rate = 0.5\n",
    "bias1 = 0.35\n",
    "bias2 = 0.6\n",
    "input_ = np.array([0.05, 0.1])\n",
    "w_0 = np.array([[0.15, 0.2], [0.25,0.30]])\n",
    "w_arr.append(w_0)\n",
    "\n",
    "\n",
    "x_0 = feedforward(input_, w_0, bias1)\n",
    "x_arr.append(x_0)\n",
    "y_0 = sigmoid(x_0)\n",
    "y_arr.append(y_0)\n",
    "\n",
    "\n",
    "#feed forward to second layer\n",
    "w_1 = np.array([[0.4, 0.45],[0.5, 0.55]])\n",
    "w_arr.append(w_1)\n",
    "x_1 = feedforward(y_0, w_1, bias2)\n",
    "x_arr.append(x_1)\n",
    "y_1 = sigmoid(x_1)\n",
    "y_arr.append(y_1)\n",
    "\n",
    "\n",
    "\n",
    "target = np.array([0.01, 0.99]).reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward (input_):\n",
    "    \n",
    "\n",
    "    for i in range(len(w_arr)): # depth of a network, 1 less than number of sets of weights\n",
    "        \n",
    "        #get previous y\n",
    "        if (i == 0): # first \"y\" is input\n",
    "            y_prev = input_         \n",
    "        else:\n",
    "            y_prev = y_arr[i-1]\n",
    "           \n",
    "        w = w_arr[i]\n",
    "        bias = bias_arr[i]\n",
    "        x = subff(y_prev, w, bias) \n",
    "        y = sigmoid(x)\n",
    "        \n",
    "        x_arr.append(x)\n",
    "        y_arr.append(y) # y is a result of activation function of x\n",
    "        \n",
    "        pd_XW_mat = partial_derivaties_XW(x, y_prev, w)\n",
    "        pd_XY_mat = partial_derivaties_XY_thru_layer(x, y_prev, w)\n",
    "        pd_YX_mat = partial_derivaties_YX_within (x, y, w)\n",
    "        \n",
    "        \n",
    "        pd_XY_thru_layer.append(pd_XY_mat)\n",
    "        pd_YX_within.append(pd_YX_mat)\n",
    "        pd_XW.append(pd_XW_mat)\n",
    "        \n",
    "        \n",
    "        \n",
    "target = np.array([0.01, 0.99]).reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(preds, target):\n",
    "    loss = cost_mat(preds, target)\n",
    "    #back propagate and update weights \n",
    "    \n",
    "    slope = loss # initialize the first partial derivative of loss\n",
    "    \n",
    "    for i in range (len(w_arr)):\n",
    "        index = -1 - i # we update weights backwards\n",
    "        \n",
    "        slope = slope.dot(pd_YX_within[index]) # slope = prev slope dot dY/dX\n",
    "        weights_gradients = slope.dot(pd_XW[index]) # gradients of weights = slope dot dX/dW\n",
    "        \n",
    "        #update new weights \n",
    "        updated_weights = update_weight(w_arr[index].ravel(), weights_gradients, learning_rate)\n",
    "        w_arr[index] = updated_weights.reshape(w_arr[index].shape)\n",
    "        #print (w_arr[index])\n",
    "        slope = slope.dot(pd_XY_thru_layer[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67097613]\n",
      " [0.99059258]]\n"
     ]
    }
   ],
   "source": [
    "# Test to see if the whole process work\n",
    "\n",
    "#feed forward to first layer\n",
    "learning_rate = 0.2\n",
    "\n",
    "input_ = np.array([0.05, 0.1])\n",
    "\n",
    "# 2 sets of weights\n",
    "x_arr = []\n",
    "y_arr = []\n",
    "w_arr = []\n",
    "\n",
    "pd_XY_thru_layer = []\n",
    "pd_YX_within = []\n",
    "pd_XW = []\n",
    "\n",
    "\n",
    "bias_arr = []\n",
    "bias1 = 0.02\n",
    "bias2 = 0.01\n",
    "bias_arr.append(bias1)\n",
    "bias_arr.append(bias2)\n",
    "w_0 = np.array([[0.15, 0.2], [0.25,0.30]])\n",
    "w_arr.append(w_0)\n",
    "w_1 = np.array([[0.4, 0.45],[0.5, 0.55]])\n",
    "w_arr.append(w_1)\n",
    "\n",
    "preds = 0\n",
    "epochs = 1000\n",
    "\n",
    "for i in range(epochs): # iterate 10 times\n",
    "    #print (i)\n",
    "    feedforward(input_)    \n",
    "    backpropagation(preds, target)\n",
    "    # after every back propagation step: x, y and partial derivatives arrays should be cleared out\n",
    "    if (i == epochs-1):\n",
    "        print(y_arr[-1])\n",
    "    x_arr.clear()\n",
    "    y_arr.clear()\n",
    "\n",
    "    pd_XY_thru_layer.clear()\n",
    "    pd_YX_within.clear()\n",
    "    pd_XW.clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
